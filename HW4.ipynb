{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markefuller/CAP4630/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHVFx3sHKlyh",
        "colab_type": "text"
      },
      "source": [
        "Homework 4\n",
        "UCF CAP 4630, Fall 2019\n",
        "Mark Fuller, ma138035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHlafqWPofzy",
        "colab_type": "text"
      },
      "source": [
        "# 0. General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gyPmKj-qe6G",
        "colab_type": "text"
      },
      "source": [
        "## Basic Terms\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1tyvN9ydUaO970PKmq0pgEg13Jz-bszgm)\n",
        "\n",
        "**Artificial Intelligence (AI)** - AI is a very broad term but can be defined as the effort to automate intellectual tasks normally performed by humans. It can be categorized in four ways: Acting humanly; Thinking humanly; Thinking rationaly; and Acting rationally. A process is rational if it always does the right thing based on the current information, given an ideal performance measure. Current development in AI centers around weak AI and strong AI. An example of weak AI is reactive machines that have no memory or experience upon which to base a decision, but instead rely on pure computational power and smart algorithms to recreate every decision every time. Strong AI involves relying on a small amount of memory and using that memory to avoid recalculating the descision when presented with a problem that has been seen before. \n",
        "\n",
        "**Machine Learning (ML)** - Machine learning is a subset of Artificial Intelligence. It attempts to embrace the aspect of how learning in accomplished in the broader AI field. In traditional programming, rules or algorithms along with data is processed by code to produce answers. In machine learning, large volumes of data along with labels that represent the answers are given to machine learning model for training. Training uses statistics to build a mathematical model relying on patterns and inference to make predictions on data not used during training. \n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1WFy1JX_Mx2SuUsbkb4rESluHkmCU6VgU)\n",
        "\n",
        "**Deep Learning** - Deep learning is a subset of machine learning that uses models based on artificial neural networks. It processes data using\n",
        "computing units, called neurons, arranged into ordered sections, called layers. These models use multiple layers to progressively extract higher level features from the raw input in order to make predictions. One major distinguishing factor between deep learning and machine learning is that deep learning doesn't require humans to perform any feature creation activities. Deep learning uses its many layers to define its own best features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEbXQYCRFahH",
        "colab_type": "text"
      },
      "source": [
        "## Learning Styles\n",
        "\n",
        "There are four learning styles used with machine learning. They are:\n",
        "\n",
        "\n",
        "1.   **Supervised learning -** All of the input data is labeled and has a specific expected result. The result could be a numeric value such as the price of a house, or a qualitative variable such as a class or tag.\n",
        "2.   **Unsupervised learning -** the input data is not labelled and the results are not known. The goals of this type of learning are often to identify unknown patterns or group similar data (clustering)   \n",
        "3.   **Self-supervised learning -** similar to supervised learning, but all of the input data does not have to be labeled. There can be a mixture of labeled and unlabeled data, and the network could ignore the labels provided. \n",
        "4.   **Reinforcement learning -** an extension of self-supervised learning that adds a feedback loop giving positive feedback when the solution performs its task correctly and negative feedback when it fails at its task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbDKyv-2olRM",
        "colab_type": "text"
      },
      "source": [
        "# 1. Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeDAKFjI7VGj",
        "colab_type": "text"
      },
      "source": [
        "## Structure of a Convent\n",
        "\n",
        "Although the computing units of deep learning are called neurons which were modeled from neurobiology, deep learning models are not models of the brain and is strictly a mathematical framework for learning representations of data. Various types of data (Images, Text, Sound, etc.) are inputs to a convolutional neural network (Convent) for processing. This is depicted in the following diagram:\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1FvfM8-lYRwV2pH2XT82wd3-syaouptgn)\n",
        "\n",
        "Each layer in the network transforms the input into representations that are increasingly different from the original image and increasingly informative about the final result that allows for a good prediction of what it represents. Encoded within the layers are information leading to a valid prediction such as edges, corners, vertical lines, horizontal lines, etc. These features were not entered as input by humans. The layers of the network determined what it considers as important towards the goal of prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhhXtiTqEY6z",
        "colab_type": "text"
      },
      "source": [
        "## Components of a Convent\n",
        "\n",
        "The components of a convolutional neural network consist of:\n",
        "\n",
        "\n",
        "*   Layers - ordered sections of neurons that takes as input one or more tensors and outputs one or more tensors. Tensors are data stored in multidimensional Numpy arrays.\n",
        "*   Input Data (X) - any information that can be expressed in a tensor. Common examples include images, video, text, sound, and tabular data.\n",
        "*   Targets (Y) - the actual labels or identification of the input data for training or the prediction made on unknown or test data\n",
        "*   Loss function - a measure of the distance between a prediction and its actual value used as a feedback signal for learning\n",
        "*   Optimizer - an algorithm that implements backpropagation to adjust the weights of all layers so that the loss is minimized\n",
        "\n",
        "The following diagram shows the relationship between the network, layers, loss function, and optimizer.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Po3PyybcUlljl5C87BJYondyalfJhx0g)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtP8sgMBf4Bl",
        "colab_type": "text"
      },
      "source": [
        "## Outline of Machine Learning Process \n",
        "\n",
        "There are several steps to defining a neural network. These can be generalized as follows:\n",
        "\n",
        "\n",
        "1.   Defining the problem and assembling a dataset\n",
        "2.   Choosing a measure of success\n",
        "3.   Deciding on an evaluation protocol\n",
        "4.   Preparing your data\n",
        "5.   Developing a model that does better than a baseline\n",
        "6.   Scaling up: developing a model that overfits\n",
        "7.   Regularizing your model and tuning your hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v2Pe_-TGjqV",
        "colab_type": "text"
      },
      "source": [
        "## An Example Convolutional Neural Network (CNN)\n",
        "\n",
        "The following steps are listed below in code cells to illustrate an example CNN. The steps are as follows:\n",
        "\n",
        "\n",
        "1.   Acquire dataset\n",
        "2.   Prepare data for training as needed by model\n",
        "3.   Define the model\n",
        "4.   Compile the model\n",
        "5.   Evaluate the model\n",
        "6.   Visualize the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVRQ6q6-EdYW",
        "colab_type": "text"
      },
      "source": [
        "## Defining a Model\n",
        "\n",
        "Keras is model-level library providing an easy interface for defining the components of a deep learning network. It does not handle the low level tensor manipulation and differentiation mathematics. Those functions are defined by a tensor library such as Tensorflow, Theano, and CNTK. \n",
        "\n",
        "An example CNN follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8D1CrM4eJol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16087dbd-50ff-4eac-893d-d53d5820b36d"
      },
      "source": [
        "# Step 1) Acquire Dataset\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOrkVwBpeOBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2) Prepare Data for Training Model\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Prepare labels if needed for Training Model - one hot encoding\n",
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anaadDCxeZpS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "58508d8c-a665-4035-9f9d-39d197885c84"
      },
      "source": [
        "# Step 3) Define the Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "# Creating a Sequential Model and adding the layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10,activation='softmax'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjEZDoVs4ANN",
        "colab_type": "text"
      },
      "source": [
        "## Layers\n",
        "\n",
        "Layers are ordered sections of neurons that takes as input one or more tensors and outputs one or more tensors. Deep learning models typically have many layers in them, sometimes hundreds of layers. The Keras framework provides different types of layers that can be used to build a model. They include:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Core layers -** including Dense, Activation, Dropout, Flatten, Input, Reshape, Permute, RepeatVector, Lambda, ActivityRegularization, Masking, SpatialDropout1D, SpatialDropout2D, and SpatialDropout3D.\n",
        "2.   **Convolutional layers -** including Conv1D, Conv2D, SeparableConv1D, SeparableConv2D, DepthwiseConv2D, Conv2DTranspose, Conv3D, Conv3DTranspose, Cropping1D, Cropping2D, Cropping3D, UpSampling1D, UpSampling2D, UpSampling3D, ZeroPadding1D, ZeroPadding2D, and ZeroPadding3D\n",
        "3.   **Pooling layers -** including MaxPooling1D, MaxPooling2D, MaxPooling3D, AveragePooling1D, AveragePooling2D, AveragePooling3D, GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling1D, GlobalAveragePooling2D, and GlobalAveragePooling3D\n",
        "4.   **Locally-connected layers -** including LocallyConnected1D and LocallyConnected2D.\n",
        "5.   **Recurrent layers -** including RNN, SimpleRNN, GRU, LSTM, ConvLSTM2D, ConvLSTM2DCell, SimpleRNNCell, GRUCell, LSTMCell, CuDNNGRU, and CuDNNLSTM\n",
        "6.   **Merge layers -** including Add, Subtract, Multiply, Average, Maximum, Minimum, Concatenate, and Dot\n",
        "7.   **Activation layers -** including LeakyReLU, PReLU, ELU, ThresholdedReLU, Softmax, and ReLU\n",
        "8.   **Normalization layers -** BatchNormalization\n",
        "9.   **Noise layers -** including GaussianNoise, GaussianDropout, and AlphaDropout\n",
        "\n",
        "The simple model defined above used a **Conv2D** to create a spatial convolution window of size 3x3 over the images of size 28x28. It then used a **MaxPooling2D** layer to down-sample the input reducing its dimensionality in part to help over-fitting. the model then used a **flatten** layer to convert the output of the convolutional part to a 1D feature vector which is needed by the classifier. The classifier portion of the network is composed of a **Dense** layer for the fully connected layer, a **dropout** layer to fight overfitting, and finally a **Dense** layer with the softmax activation function limiting the predictions to the number of classifications desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCMIt63UooRa",
        "colab_type": "text"
      },
      "source": [
        "# 2. Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmY7utO7gP-",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers\n",
        "\n",
        "An optimizer is one of the two arguments required for compiling a Keras model. It is the mechanism through which the network will update itself based on the data it sees and its loss function. It implements a specific variant of stochastic gradient descent (SGD). There are many different optimizers supported by the Keras framework. They include:\n",
        "\n",
        "\n",
        "1.   **Adagrad -** Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate. It is recommended to leave the parameters of this optimizer at their default values.\n",
        "2.   **Adadelta -** Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. \n",
        "3.   **RMSprop -** Divides the gradient by a running average of its recent magnitude. It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned).\n",
        "4.   **Adam - ** An algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
        "5.   **Adamax -** It is a variant of Adam based on the infinity norm.\n",
        "6.   **Nadam -** Nesterov Adam optimizer, much like Adam, is essentially RMSprop with momentum, Nadam is RMSprop with Nesterov momentum. It is recommended to leave the parameters of this optimizer at their default values.\n",
        "7.   **SGD -** Stochastic gradient descent optimizer which includes support for momentum, learning rate decay, and Nesterov momentum.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO3W3fURsVPn",
        "colab_type": "text"
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "A loss function (or objective function, or optimization score function) is one of the two parameters required to compile a model. It represents how the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction. There are many loss functions provided by the Keras framework. They include:\n",
        "\n",
        "\n",
        "1.   **mean_squared_error**\n",
        "2.   **mean_absolute_error**\n",
        "3.   **mean_absolute_percentage_error**\n",
        "4.   **mean_squared_logarithmic_error**\n",
        "5.   **squared_hinge**\n",
        "6.   **hinge**\n",
        "7.   **categorical_hinge**\n",
        "8.   **logcosh**\n",
        "9.   **huber_loss**\n",
        "10.  **categorical_crossentropy**\n",
        "11.  **sparse_categorical_crossentropy**\n",
        "12.  **binary_crossentropy**\n",
        "13.  **kullback_leibler_divergence**\n",
        "14.  **poisson**\n",
        "15.  **cosine_proximity**\n",
        "16.  **is_categorical_crossentropy**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65lje_bDyrlo",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "A metric is a function that is used to judge the performance of your model. it is usually specified in the compilation of a model, but it not a required parameter. A metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model. You may use any of the loss functions as a metric function. Additional metric functions provided by the Keras framework include:\n",
        "\n",
        "\n",
        "1.   **accuracy**\n",
        "2.   **binary_accuracy**\n",
        "3.   **categorical_accuracy**\n",
        "4.   **sparse_categorical_accuracy**\n",
        "5.   **top_k_categorical_accuracy**\n",
        "6.   **sparse_top_k_categorical_accuracy**\n",
        "7.   **cosine_proximity**\n",
        "8.   **clone_metric**\n",
        "9.   **clone_metrics**\n",
        "\n",
        "It is also possible to create custom metric function instead of using the functions supplied by Keras. The next code cell shows compilation of a model using the two required parameters and the optional metrics parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfXqJnDdedpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4) Compile the Model\n",
        "model.compile(optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tqF6yB3otFB",
        "colab_type": "text"
      },
      "source": [
        "# 3. Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCwahSTX_a8h",
        "colab_type": "text"
      },
      "source": [
        "## Underfitting\n",
        "\n",
        "Underfitting is the inability of the model to predict well the labels of the data it was trained on. The most common causes are:\n",
        "\n",
        "\n",
        "1.   The model is too simple for the data\n",
        "2.   The features are not informative enough\n",
        "3.   There is not enough data to train on\n",
        "\n",
        "In deep learning, a simple solution for underfitting is to add additional layers to the model provided that there is sufficient data to train on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6yHrIb7nGM",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting\n",
        "\n",
        "Overfitting is a central problem of machine learning. It is when the model contains more parameters than can be justified by the data. It results in a network model performing excellent on the training data, but unable to replicate the accuracy on test data. There are several methods to combat overfitting. They include:\n",
        "\n",
        "\n",
        "1.   Reduce the networks size by lowering the trainable parameters. \n",
        "2.   Adding weight regularization by forcing the model to only use small values for its weights which makes the distribution of weight values more regular.\n",
        "3.   Adding dropout layers randomly drops out a number of output features of the layer during training\n",
        "4.   Add more training data. \n",
        "5.   Data augmentation to create variance in the input data using techniques such as cropping, padding, horizontal flipping, rotating, zooming, and other methods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JbbuzfJExGI",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "The next code cell shows an example of actually training a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui6WAAVYehOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "c2714e1f-88c1-4962-bf31-4014a76b23c3"
      },
      "source": [
        "# Step 5) Train the Model\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "history = model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images,test_labels))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 8s 139us/step - loss: 0.2803 - acc: 0.9181 - val_loss: 0.1034 - val_acc: 0.9693\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1063 - acc: 0.9683 - val_loss: 0.0700 - val_acc: 0.9780\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0713 - acc: 0.9784 - val_loss: 0.0566 - val_acc: 0.9823\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 5s 79us/step - loss: 0.0549 - acc: 0.9833 - val_loss: 0.0599 - val_acc: 0.9806\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.0540 - val_acc: 0.9836\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0358 - acc: 0.9879 - val_loss: 0.0499 - val_acc: 0.9849\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0296 - acc: 0.9911 - val_loss: 0.0457 - val_acc: 0.9863\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0256 - acc: 0.9915 - val_loss: 0.0531 - val_acc: 0.9842\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0208 - acc: 0.9932 - val_loss: 0.0461 - val_acc: 0.9850\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0183 - acc: 0.9936 - val_loss: 0.0487 - val_acc: 0.9859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhz3yg6LE3bE",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Results of Training\n",
        "\n",
        "After training a model, it is common to visualize the results of the training using graphs to plot losses and accuracies. It is also possible to produce representations of the inner workings of a model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUvbmgQwjkc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "15be2666-35ad-4595-94bc-bc1a7183f399"
      },
      "source": [
        "# Step 6) Visualizing the Results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss']\n",
        "test_loss_values = history_dict['val_loss']\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.plot(epochs_range, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs_range, test_loss_values, 'ro', label='Test loss')\n",
        "plt.title('Training and test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEE911iOnjRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4c28bbae-952e-4f9c-f98d-80752e0714be"
      },
      "source": [
        "acc_values = history_dict['acc']\n",
        "test_acc_values = history_dict['val_acc']\n",
        "\n",
        "plt.plot(epochs_range, acc_values, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs_range, test_acc_values, 'ro', label='Test accuracy')\n",
        "plt.title('Training and test accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5hU1Znv8e+PiwKCioCaCNIkOmor\nl2BLQoiiqARPNESMgtPRiHEwiSaOJ5rgUY8znKjJaI6JCWPCODg6EtEx0dFkDOGqyVETWrkFEESG\nSyNqg9he0EjDe/7Yu9ui3Q3V0tVV0L/P89RTtde+vbUb9ltrrb3XVkRgZmbWWLtiB2BmZqXJCcLM\nzDI5QZiZWSYnCDMzy+QEYWZmmZwgzMwskxOEtTpJ7SW9LenIlly2mCQdJcnXjNs+xQnCdis9Qde/\ndkh6N2e6srnbi4jtEdE1Ita15LKlTFK1pFNbYDuXSZq35xGZ7V6HYgdgpS8iutZ/lrQGuCwiZjW1\nvKQOEVHXGrFZ6ZHUPiK2FzsO23OuQdgek/R9SQ9KekDSW8BXJA2V9KykNyRtlHSnpI7p8h0khaSy\ndPr+dP4Tkt6S9Iykfs1dNp1/lqSVkmol/VTS/5N0SRNx5xPj5ZJWSdoi6c6cddtLukPSZkmrgVG7\nOD4PAB8HnkhrXf8zLR+Ws/+Fkk7JWedrktak33G1pHGS+gM/A05Ot7Opif1dJml5uu5Lki5rNH9M\nur830+82Mi3vIenf0mOxRdKvcrY3L2f9rL/JZEm/k/ROGt8Xc/axTtKNjWI4Jf3utZLWS7oo/Xu8\nLKldznIXSHquqWNrBRYRfvmV9wtYA5zRqOz7wPvAOSQ/OjoDJwGfJqmlfgJYCVyZLt8BCKAsnb4f\n2ARUAB2BB4H7P8KyhwJvAaPTef8T2AZc0sR3ySfG/wQOAsqA1+u/O3AlsBToDfQAnkr+OzV53KqB\nU3Om+wCbgc+nx2xU+r16AAcCtcDR6bIfA8rTz5cB83bzNzon/T4CRgDvAgPSeZ8F3gBOT/fbBzgm\nnTcD+CXQPT1+p2Tts4m/yRZgaLrN/dP9Hp9OD0y/29np8v2At4EL0m31BAal81YAZ+bs63HgqmL/\nu2+rL9cgrKX8MSIej4gdEfFuRMyPiD9FRF1ErAamAMN3sf7DEVEVEduAacCgj7Ds2cDCiPjPdN4d\nJCemTHnGeGtE1EbEGmBezr4uAO6IiOqI2Az8YBfxZrkYeCwiZqTH7HfAIj6oiQRwgqROEbExIpbl\nu+H077A6EnOA2cDJ6eyvAf8SEbPT/a6PiBWS+pAkjW9ExJaI2BYRTzXj+zwSEc+k2/xrRMyJiKXp\n9CJgOh8c268AT0TEQ+mx3xQRC9N596XzkdQzjemBZsRhLcgJwlrK+twJScdK+q2kVyS9CUwi+aXY\nlFdyPm8Fuja14C6W/XhuHBERJL/cM+UZY177AtbuIt4sfYEL0+alNyS9AXwG+HhEvAlcCFwBvCLp\nN5L+Jt8NSzpb0p8kvZ5udyQffK8+wEsZq/UBNkVEbTO/R73Gf/+hkuZJqpFUS1IL2V0MAP8OjJbU\nGRgHzI2I1z5iTLaHnCCspTS+xPMXwF+AoyLiQOB/kzR5FNJGkiYfACQJOGIXy+9JjBtJTnT1dncZ\nbuPjsx64JyIOznkdEBG3AUTEExFxBknz0qo01qzt7CQ9sT4M3AocFhEHA7/ng++1HvhkxqrrgZ6S\nDsyY9w7QJWf68Dy+33TgV0CfiDgIuDuPGIjkarXngC8BF5EkDCsSJwgrlG4k7ejvSDoOuLwV9vkb\nYLCkcyR1AK4CehUoxoeAv5d0hKQewPd2s/yrJP0C9f4dOFfSmWmHdydJp0n6uKSPpd+hC0nfzjvA\njpzt9K7vTM+wP7AfUANsl3Q2STNNvX8FLkv31U5Sb0nHRMR6YBYwWdLBkjrmdJovAgZI6p8moJvy\nOD7dgNcj4j1JnyGpDdS7Hxgl6by0w7unpIE58+8DrgOOJekDsiJxgrBC+Q7wVZJO41+QdCYXVES8\nCowF/i9JB/AngQXAXwsQ410kbftLgPkkv9p35RbgH9PmpL9P+zTOBW4kOZmvS+NpB7QHriWppWwm\n6Vi+It3OTOBF4FVJr9BIRLwBXA08QtKp/mWSxFk//2ng74A7SZLjXD6oCX0lfV9Jkoi+la6zLI1/\nHkkncj59E98AblVyVdv/Ikmo9TH8N0lH+vfSGJ8H+ues+yuSZPpwRLybx76sQJQ005rteyS1B14G\nvhwRfyh2PJaftGnwv0muPptX5HDaNNcgbJ8iaVTaRLI/ya/zbcCfixyWNc8FJLW+J4sdSFvnO6lt\nX/M5kmv5O5Dcp3BuRDTVxGQlRtIfgaOBynDzRtG5icnMzDK5icnMzDLtM01MPXv2jLKysmKHYWa2\nV3nuuec2RUTm5eD7TIIoKyujqqqq2GGYme1VJDU5CoCbmMzMLJMThJmZZXKCMDOzTE4QZmaWyQnC\nzMwyOUGYme2lpk2DsjJo1y55nzatZbfvBGFm1kyFPjHnG8OECbB2LUQk7xMmtGwsThBmZs3QGifm\nfFx/PWzdunPZ1q1JeUtxgjAza4bWODHnY9265pV/FE4QZmbN0Bon5nwc2cRDbpsq/yicIMzMmqE1\nTsz5uPlm6NJl57IuXZLyluIEYWZ7jVLoHG6NE3M+KithyhTo2xek5H3KlKS8pThBmFlein1yLpXO\n4dY4MTcnljVrYMeO5L2lY9hnHhhUUVERHs3VrDDqT865nbNdurTuibGsLEkKjfXtm5wc7aOR9FxE\nVGTNK2gNIn0+8ApJqyRNzJjfV9JsSYslzZPUO2feDyX9JX2NLWScZrZrpXDlTql0DrclBUsQktoD\nk4GzgHLgQknljRa7HbgvIgYAk4Bb03W/AAwGBgGfBq6RdGChYjWzXSuFk3OpdA63JYWsQQwBVkXE\n6oh4H5gOjG60TDkwJ/08N2d+OfBURNRFxDvAYmBUAWM1s10ohZNzqXQOtyWFTBBHAOtzpqvTslyL\ngDHp53OBbpJ6pOWjJHWR1BM4DehTwFjNbBdK4eRcSp3DbUWxHzl6DfAzSZcATwEbgO0R8XtJJwFP\nAzXAM8D2xitLmgBMADjS9Uyzgqk/CV9/fdKsdOSRSXJo7ZNzZaUTQmsq2FVMkoYC/xARn0+nrwOI\niFubWL4r8EJE9M6Y90vg/oj4r6b256uYzMyar1hXMc0HjpbUT9J+wDjgsUaB9ZRUH8N1wNS0vH3a\n1ISkAcAA4PcFjNWsZBX7/gNruwrWxBQRdZKuBGYA7YGpEbFU0iSgKiIeA04FbpUUJE1MV6SrdwT+\nIAngTeArEVFXqFjNSlXj+w/qbw4DN7VY4flGObMS5pvDrNCKdqOcme2ZUrj/wNouJwizJpRC238p\n3H/QoBQOiLUqJwizDKUyMFwp3H8AlM4BsZ0VOGm7D8IsQym1/U+bVvz7D0rqgFiihUZQ3FUfhBOE\nWYZ27ZIfyo1JydDKbY4PSOlpoaTtTmqzZnLbfyMldUBKQCn8TVrhCgYnCLMMbvtvpGQOSAkolb9J\nayTtiNgnXieeeGKYtaT774/o2zdCSt7vv78IQfTtG5GchnZ+9e3b+rGUwgEphRhK5W9y//0RXbrs\nHEOXLs0+JiQ3LmeeV12DMGtCJdNYQxk7aMcayqhk32xGyFuhn2+5O6Xyy71U/iatMLytE4SVnFJo\n3i2Zk5Hb/j9QCo+1g9L6mxQ4aTtBWEkplfNyyZyM3Pb/gVL55d6G/iZOEFZSSuW8XDInIz8l5wOl\n8su9Df1NnCDsAyXQtlMq5+WSORlB8dv+S0Up/XJvI38TJwhLlEjbTsmcl0vpZGSJNvTLvVQ4QVii\nRNp2Sua87JNRaWojv9xLhYfasEQJDaVQEmMPmbURHmrDdq9k2nZK5P4DM3OCsFSptO2USF+ImRU4\nQUgaJWmFpFWSJmbM7ytptqTFkuZJ6p0z758kLZW0XNKdSh9QbQVSKm3uJdIXYmYF7IOQ1B5YCZwJ\nVAPzgQsjYlnOMv8B/CYi7pU0AhgfERdJ+ixwG3BKuugfgesiYl5T+3MfxD6ihPpCzNqCYvVBDAFW\nRcTqiHgfmA6MbrRMOTAn/Tw3Z34AnYD9gP2BjsCrBYzVSkUJ9YWYtXWFTBBHAOtzpqvTslyLgDHp\n53OBbpJ6RMQzJAljY/qaERHLG+9A0gRJVZKqampqWvwLWBGUSl+ImRW9k/oaYLikBcBwYAOwXdJR\nwHFAb5KkMkLSyY1XjogpEVERERW9evVqzbj3SSVwI3Xp9IWYWUETxAagT85077SsQUS8HBFjIuJT\nwPVp2RsktYlnI+LtiHgbeAIYWsBYi6sEzswldfGQb4YyKwmFTBDzgaMl9ZO0HzAOeCx3AUk9JdXH\ncB0wNf28jqRm0UFSR5LaxYeamPYJJXJm9sVDZtZYwRJERNQBVwIzSE7uD0XEUkmTJH0xXexUYIWk\nlcBhQH1D88PAS8ASkn6KRRHxeKFiLaoSOTOXzCB5ZlYyPNRGsZXIZZ1lZUnlpbG+fZNWHjPbN3mo\njVJWIpd1+uIhM2vMCaLYSuTM7IuHzKyxDsUOoM2rPwOXwPCllZVOCGb2ASeIUuAzs5mVIDcxmZlZ\nJicIMzPL5ARhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlskJwszMMjlBmJlZJicIMzPL5ARhZmaZ\nnCDMzCyTE4SZmWUqaIKQNErSCkmrJE3MmN9X0mxJiyXNk9Q7LT9N0sKc13uSvlTIWM3MbGcFSxCS\n2gOTgbOAcuBCSeWNFrsduC8iBgCTgFsBImJuRAyKiEHACGAr8PtCxWpmZh9WyBrEEGBVRKyOiPeB\n6cDoRsuUA3PSz3Mz5gN8GXgiIrYWJMpp05IHMrdrl7xPm1aQ3ZiZ7W0KmSCOANbnTFenZbkWAWPS\nz+cC3ST1aLTMOOCBrB1ImiCpSlJVTU1N8yOcNg0mTIC1ayEieZ8wwUnCzIzid1JfAwyXtAAYDmwA\nttfPlPQxoD8wI2vliJgSERURUdGrV6/m7/3662Fro4rJ1q1JeStyJcbMSlEhHzm6AeiTM907LWsQ\nES+T1iAkdQXOi4g3cha5AHgkIrYVJMJ165pXXgD1lZj6PFVfiQE/hdTMiquQNYj5wNGS+knaj6Sp\n6LHcBST1lFQfw3XA1EbbuJAmmpdaxJFHNq+8AEqkEmNm9iEFSxARUQdcSdI8tBx4KCKWSpok6Yvp\nYqcCKyStBA4Dbq5fX1IZSQ3kyULFyM03Q5cuO5d16ZKUt5ISqMSYmWVSRBQ7hhZRUVERVVVVzV9x\n2rTk5/q6dUnN4eabW7Vtp6wsaVZqrG9fWLOm1cIwszZK0nMRUZE1r9id1MVXWZmciXfsSN5bueG/\nBCoxZmaZnCCKrLISpkxJagxS8j5lijuozaz4CnkVk+WpstIJwcxKj2sQZmaWyQnCzMwyOUGYmVkm\nJwgzM8vkBGFmZpmcIMzMLJMThJmZZXKCMDOzTLtNEJK+Jal7awRjZmalI58axGHAfEkPSRolSYUO\nyszMim+3CSIibgCOBv4VuAR4UdItkj5Z4NjMzKyI8uqDiGRM8FfSVx3QHXhY0j8VMDYzMyui3Q7W\nJ+kq4GJgE3A3cG1EbEufBPci8N3ChmhmZsWQz2iuhwBjImKnx9pExA5JZxcmLDMzK7Z8mpieAF6v\nn5B0oKRPA0TE8l2tmHZqr5C0StLEjPl9Jc2WtFjSPEm9c+YdKen3kpZLWpY+gtTMzFpJPgniLuDt\nnOm307JdktQemAycBZQDF0oqb7TY7cB9ETEAmATcmjPvPuC2iDgOGAK8lkesZmbWQvJJEIqcB1dH\nxA7ya5oaAqyKiNUR8T4wHRjdaJlyYE76eW79/DSRdIiImek+346IrXns08zMWkg+CWK1pG9L6pi+\nrgJW57HeEcD6nOnqtCzXImBM+vlcoJukHsDfAG9I+rWkBZJuS2skO5E0QVKVpKqampo8QjIzs3zl\nkyC+DnwW2EBykv80MKGF9n8NMFzSAmB4uo/tJDWUk9P5JwGfILkHYycRMSUiKiKiolevXi0UkpmZ\nQR5NRRHxGjDuI2x7A9AnZ7p3Wpa77ZdJaxCSugLnRcQbkqqBhRGxOp33KPAZkpv1zMysFeRzH0Qn\n4GvA8UCn+vKIuHQ3q84HjpbUjyQxjAP+ttG2ewKvp/0a1wFTc9Y9WFKviKgBRgBVeX0jMzNrEfk0\nMf07cDjweeBJkprAW7tbKSLqgCuBGcBy4KGIWCppkqQvpoudCqyQtJJkzKeb03W3kzQvzZa0BBDw\nL834XmZmtoeUc4FS9gLSgoj4lKTFETFAUkfgDxHxmdYJMT8VFRVRVeVKhplZc0h6LiIqsublU4PY\nlr6/IekE4CDg0JYKzszMSlM+9zNMSZ8HcQPwGNAVuLGgUZmZWdHtMkGkA/K9GRFbgKdILjc1M7M2\nYJdNTOnVRR6t1cysDcqnD2KWpGsk9ZF0SP2r4JGZmVlR5dMHMTZ9vyKnLHBzk5nZPi2fO6n7tUYg\nZmZWWvK5k/rirPKIuK/lwzEzs1KRTxPTSTmfOwGnA8+TPK/BzMz2Ufk0MX0rd1rSwSTPdjAzs31Y\nPlcxNfYO4H4JM7N9XD59EI+TXLUESUIpBx4qZFBmZlZ8+fRB3J7zuQ5YGxHVBYrHzMxKRD4JYh2w\nMSLeA5DUWVJZRKwpaGRmZlZU+fRB/AewI2d6e1pmZmb7sHwSRIeIeL9+Iv28X+FCMjOzUpBPgqjJ\neQIckkYDmwoXkpmZlYJ8EsTXgf8laZ2kdcD3gMvz2bikUZJWSFolaWLG/L6SZktaLGmepN4587ZL\nWpi+Hsv3C5mZWcvI50a5l4DPSOqaTr+dz4YltQcmA2cC1cB8SY9FxLKcxW4H7ouIeyWNAG4FLkrn\nvRsRg/L/KmZm1pJ2W4OQdIukgyPi7Yh4W1J3Sd/PY9tDgFURsTrtt5gOjG60TDkwJ/08N2O+mZkV\nST5NTGdFxBv1E+nT5f5HHusdAazPma5Oy3ItAsakn88FuknqkU53klQl6VlJX8ragaQJ6TJVNTU1\neYRkZmb5yidBtJe0f/2EpM7A/rtYvjmuAYZLWgAMBzaQXEYL0DciKoC/BX4s6ZONV46IKRFREREV\nvXr1aqGQzMwM8rtRbhowW9I9gIBLgHvzWG8D0Cdnunda1iAiXiatQaR9HOfV11YiYkP6vlrSPOBT\nwEt57NfMzFrAbmsQEfFD4PvAccAxwAygbx7bng8cLamfpP2AccBOVyNJ6impPobrgKlpeff6Wouk\nnsAwILdz28zMCizf0VxfJRmw73xgBLB8dytERB1wJUlCWQ48FBFLJU3Kua/iVGCFpJXAYcDNaflx\nQJWkRSSd1z9odPWTmZkVmCIie4b0N8CF6WsT8CBwTUTkU3todRUVFVFVVVXsMMzM9iqSnkv7ez9k\nV30QLwB/AM6OiFXphq4uQHxmZlaCdtXENAbYCMyV9C+STifppDYzszagyQQREY9GxDjgWJJ+gL8H\nDpV0l6SRrRWgmZkVRz5XMb0TEb+MiHNILlVdQDIek5mZ7cOa9UzqiNiS3px2eqECMjOz0tCsBGFm\nZm2HE4SZmWVygjAzs0xOEGZmlskJwszMMjlBmJlZJicIMzPL5ARhZmaZnCDMzCyTE4SZmWVygjAz\ns0xOEGZmlqmgCULSKEkrJK2SNDFjfl9JsyUtljRPUu9G8w+UVC3pZ4WM08zMPqxgCUJSe2AycBZQ\nDlwoqbzRYrcD90XEAGAScGuj+f8HeKpQMZqZWdMKWYMYAqyKiNUR8T4wHRjdaJlyYE76eW7ufEkn\nAocBvy9gjGZm1oRCJogjgPU509VpWa5FJI82BTgX6Caph6R2wI+AawoYn5mZ7UKxO6mvAYZLWgAM\nBzYA24FvAv8VEdW7WlnSBElVkqpqamoKH62ZWRvSoYDb3gD0yZnunZY1iIiXSWsQkroC50XEG5KG\nAidL+ibQFdhP0tsRMbHR+lOAKQAVFRVRsG9iZtYGFTJBzAeOltSPJDGMA/42dwFJPYHXI2IHcB0w\nFSAiKnOWuQSoaJwczMyssArWxBQRdcCVwAxgOfBQRCyVNEnSF9PFTgVWSFpJ0iF9c6HiMTOz5lHE\nvtEyU1FREVVVVcUOw8xsryLpuYioyJpX7E5qMzMrUU4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFm\nZpmcIMzMLJMThJmZZXKCMDOzTE4QZmaWyQnCzMwyOUGYmVkmJwgzM8vkBGFmZpmcIMzMLJMThJmZ\nZXKCMDOzTE4QZmaWqaAJQtIoSSskrZI0MWN+X0mzJS2WNE9S75zy5yUtlLRU0tcLGaeZmX1YwRKE\npPbAZOAsoBy4UFJ5o8VuB+6LiAHAJODWtHwjMDQiBgGfBiZK+nihYjUzsw8rZA1iCLAqIlZHxPvA\ndGB0o2XKgTnp57n18yPi/Yj4a1q+f4HjNDOzDIU88R4BrM+Zrk7Lci0CxqSfzwW6SeoBIKmPpMXp\nNn4YES833oGkCZKqJFXV1NS0+BcwM2vLiv3L/BpguKQFwHBgA7AdICLWp01PRwFflXRY45UjYkpE\nVERERa9evVozbjOzfV4hE8QGoE/OdO+0rEFEvBwRYyLiU8D1adkbjZcB/gKcXMBYzcyskQ4F3PZ8\n4GhJ/UgSwzjgb3MXkNQTeD0idgDXAVPT8t7A5oh4V1J34HPAHQWM1azN2bZtG9XV1bz33nvFDsVa\nQadOnejduzcdO3bMe52CJYiIqJN0JTADaA9MjYilkiYBVRHxGHAqcKukAJ4CrkhXPw74UVou4PaI\nWFKoWM3aourqarp160ZZWRmSih2OFVBEsHnzZqqrq+nXr1/e6xWyBkFE/BfwX43K/nfO54eBhzPW\nmwkMKGRsZm3de++95+TQRkiiR48eNPdinmJ3UptZETk5tB0f5W/tBGFmZpmcIMwsL9OmQVkZtGuX\nvE+btmfb27x5M4MGDWLQoEEcfvjhHHHEEQ3T77//fl7bGD9+PCtWrNjlMpMnT2bangbbRhW0D8LM\n9g3TpsGECbB1azK9dm0yDVBZ+dG22aNHDxYuXAjAP/zDP9C1a1euueaanZaJCCKCdu2yf8vec889\nu93PFVdcsdtlSk1dXR0dOhT/9OwahJnt1vXXf5Ac6m3dmpS3tFWrVlFeXk5lZSXHH388GzduZMKE\nCVRUVHD88cczadKkhmU/97nPsXDhQurq6jj44IOZOHEiAwcOZOjQobz22msA3HDDDfz4xz9uWH7i\nxIkMGTKEY445hqeffhqAd955h/POO4/y8nK+/OUvU1FR0ZC8ct10002cdNJJnHDCCXz9618nIgBY\nuXIlI0aMYODAgQwePJg1a9YAcMstt9C/f38GDhzI9enBqo8Z4JVXXuGoo44C4O677+ZLX/oSp512\nGp///Od58803GTFiBIMHD2bAgAH85je/aYjjnnvuYcCAAQwcOJDx48dTW1vLJz7xCerq6gDYsmXL\nTtMflROEme3WunXNK99TL7zwAldffTXLli3jiCOO4Ac/+AFVVVUsWrSImTNnsmzZsg+tU1tby/Dh\nw1m0aBFDhw5l6tSpmduOCP785z9z2223NSSbn/70pxx++OEsW7aMG2+8kQULFmSue9VVVzF//nyW\nLFlCbW0tv/vd7wC48MILufrqq1m0aBFPP/00hx56KI8//jhPPPEEf/7zn1m0aBHf+c53dvu9FyxY\nwK9//Wtmz55N586defTRR3n++eeZNWsWV199NQCLFi3ihz/8IfPmzWPRokX86Ec/4qCDDmLYsGEN\n8TzwwAOcf/75e1wLcYIws9068sjmle+pT37yk1RUVDRMP/DAAwwePJjBgwezfPnyzATRuXNnzjrr\nLABOPPHEhl/xjY0ZM+ZDy/zxj39k3LhxAAwcOJDjjz8+c93Zs2czZMgQBg4cyJNPPsnSpUvZsmUL\nmzZt4pxzzgGSG9K6dOnCrFmzuPTSS+ncuTMAhxxyyG6/98iRI+nevTuQJLKJEycyYMAARo4cyfr1\n69m0aRNz5sxh7NixDdurf7/ssssamtzuuecexo8fv9v97Y4ThJnt1s03Q5cuO5d16ZKUF8IBBxzQ\n8PnFF1/kJz/5CXPmzGHx4sWMGjUq8+7v/fbbr+Fz+/btm2xe2X///Xe7TJatW7dy5ZVX8sgjj7B4\n8WIuvfTSj3QXeocOHdixYwfAh9bP/d733XcftbW1PP/88yxcuJCePXvucn/Dhw9n5cqVzJ07l44d\nO3Lsscc2O7bGnCDMbLcqK2HKFOjbF6TkfcqUj95B3Rxvvvkm3bp148ADD2Tjxo3MmDGjxfcxbNgw\nHnroIQCWLFmSWUN59913adeuHT179uStt97iV7/6FQDdu3enV69ePP7440By0t+6dStnnnkmU6dO\n5d133wXg9ddfB6CsrIznnnsOgIcf/tB9wg1qa2s59NBD6dChAzNnzmTDhmQouxEjRvDggw82bK/+\nHeArX/kKlZWVLVJ7ACcIM8tTZSWsWQM7diTvrZEcAAYPHkx5eTnHHnssF198McOGDWvxfXzrW99i\nw4YNlJeX84//+I+Ul5dz0EEH7bRMjx49+OpXv0p5eTlnnXUWn/70pxvmTZs2jR/96EcMGDCAz33u\nc9TU1HD22WczatQoKioqGDRoEHfckQwnd+211/KTn/yEwYMHs2XLliZjuuiii3j66afp378/06dP\n5+ijjwaSJrDvfve7nHLKKQwaNIhrr722YZ3Kykpqa2sZO3ZsixwX1ffC7+0qKiqiqqqq2GGY7TWW\nL1/OcccdV+wwSkJdXR11dXV06tSJF198kZEjR/Liiy+WxKWmzTF9+nRmzJjR5OW/WX9zSc9FREXW\n8nvXtzczK4C3336b008/nbq6OiKCX/ziF3tdcvjGN77BrFmzGq5kagl71xEwMyuAgw8+uKFfYG91\n1113tfg23QdhZmaZnCDMzCyTE4SZmWVygjAzs0wFTRCSRklaIWmVpIkZ8/tKmi1psaR56bOokTRI\n0jOSlqbzWuaiXjP76Fp4vO+WGO4bYOrUqbzyyit7FItlK9hVTJLaA5OBM4FqYL6kxyIi9xbF24H7\nIuJeSSOAW4GLgK3AxRHxouHmuNkAAAp9SURBVKSPA89JmhERbxQqXjPbhQKM953PcN/5mDp1KoMH\nD+bwww//SHG0hFIZnrulFbIGMQRYFRGrI+J9YDowutEy5cCc9PPc+vkRsTIiXkw/vwy8BvQqYKxm\ntiutOd43cO+99zJkyBAGDRrEN7/5TXbs2EFdXR0XXXQR/fv354QTTuDOO+/kwQcfZOHChYwdOzaz\n5vHzn/+ck046iYEDB3L++ec3DHvxyiuvMHr06IYhs//0pz8BHx5GG5LhKx599NGGbXbt2hWAWbNm\nceqpp3L22WfTv39/AM455xxOPPFEjj/+eO6+++6GdX77298yePBgBg4cyMiRI9mxYwdHHXVUwzAZ\n27dv5xOf+MROw2aUhPoHcrT0C/gycHfO9EXAzxot80vgqvTzGCCAHo2WGQIsB9pl7GMCUAVUHXnk\nkWFm+Vu2bFn+C0sR8OGX1CKx3HTTTXHbbbdFRMSSJUti9OjRsW3btoiI+Lu/+7uYNm1aPPvsszFq\n1KiGdbZs2RIREcOGDYsFCxZkbnfTpk0Nn7/3ve/FP//zP0dExJgxY+KnP/1pRERs27YtamtrY+HC\nhXHMMcfE5s2bIyIa3isrK+ORRx5p2M4BBxwQEREzZ86MAw44INauXdswr36dd955J4477rh4/fXX\nY+PGjdGnT59Ys2bNTsvccMMNDTH89re/jQsuuKCZR635sv7mQFU0cR4vdif1NcBwSQuA4cAGYHv9\nTEkfA/4dGB8ROxqvHBFTIqIiIip69XIFw6xgWnG871mzZjF//vyGMYyefPJJXnrpJY466ihWrFjB\nt7/9bWbMmPGhsZKyLF68mJNPPrlhPKOlS5cCMG/ePC6//HIgGV31wAMPbHIY7V0ZOnQoR+Ycgzvu\nuKPhgUXV1dW89NJLPPPMM5x22mn07dt3p+1+7Wtf49577wWSZrKWGmCvJRUyQWwA+uRM907LGkTE\nyxExJiI+BVyflr0BIOlA4LfA9RHxbKGCbOnn7Jrtk1pxvO+I4NJLL2XhwoUsXLiQFStWcOONN9Kj\nR4+GE/7kyZMbTvC7cvHFF3PXXXexZMkSbrjhhp2Gy5aUVzy5w3Nv3759pyHCc4fnnjVrFk899RTP\nPvssixYtYsCAAbscnrusrIzu3bszd+5cFixYwMiRI/OKpzUVMkHMB46W1E/SfsA44LHcBST1lFQf\nw3XA1LR8P+ARkg7spsfD3UP1/W5r1yb15fp+NycJs0ZacbzvM844g4ceeohNmzYBydVO69ato6am\nhojg/PPPZ9KkSTz//PMAdOvWjbfeeitzW++88w6HH34427Zt45e//GVD+WmnncbPf/5zIDnp1z/e\nM2sY7dzhuR955BG2b99OltraWg455BA6d+7M0qVLmT9/PgCf/exnmTt3LmvXrt1pu5DUIiorKxk3\nblyTz90upoJFFBF1wJXADJI+hIciYqmkSZK+mC52KrBC0krgMKD+58gFwCnAJZIWpq9BLR1jK/e7\nme3dWmm87/79+3PTTTdxxhlnNDxN7dVXX2X9+vUNQ1yPHz+eW265BYDx48dz2WWXZXZST5o0iZNO\nOolhw4ZRXl7eUP6zn/2MGTNm0L9/fyoqKnjhhReaHEb78ssvZ+bMmQwcOJAFCxY0PHCosS984Qts\n3bqV8vJybrjhhobhwA877DDuuusuRo8ezcCBA6nMOW7nnnsutbW1XHLJJS15CFtMmx7uu127pObQ\nmJT8HzDbl3m47+J79tlnue6665g7d26r7K+5w32XXp2mFbX2c3bNzOrdfPPNjB07tqEmVIradIJo\n7efsmpnVu/7661m7di1Dhw4tdihNatMJopjP2TUrBftKE7Pt3kf5W+9794Y3U2WlE4K1TZ06dWLz\n5s306NEj70s+be8UEWzevJlOnTo1a702nyDM2qrevXtTXV1NTU1NsUOxVtCpUyd69+7drHWcIMza\nqI4dO9KvX79ih2ElrE33QZiZWdOcIMzMLJMThJmZZdpn7qSWVAOsLXYce6gnsKnYQZQQH4+d+Xh8\nwMdiZ3tyPPpGROZw2PtMgtgXSKpq6pb3tsjHY2c+Hh/wsdhZoY6Hm5jMzCyTE4SZmWVygigtU4od\nQInx8diZj8cHfCx2VpDj4T4IMzPL5BqEmZllcoIwM7NMThAlQFIfSXMlLZO0VNJVxY6p2CS1l7RA\n0m+KHUuxSTpY0sOSXpC0XFLpPkCgFUi6Ov1/8hdJD0hq3hClezlJUyW9JukvOWWHSJop6cX0vXtL\n7MsJojTUAd+JiHLgM8AVksp3s86+7iqSZ5kb/AT4XUQcCwykDR8XSUcA3wYqIuIEoD0wrrhRtbp/\nA0Y1KpsIzI6Io4HZ6fQec4IoARGxMSKeTz+/RXICOKK4URWPpN7AF4C7ix1LsUk6CDgF+FeAiHg/\nIt4oblRF1wHoLKkD0AV4ucjxtKqIeAp4vVHxaODe9PO9wJdaYl9OECVGUhnwKeBPxY2kqH4MfBfY\nUexASkA/oAa4J21yu1vSAcUOqlgiYgNwO7AO2AjURsTvixtVSTgsIjamn18BDmuJjTpBlBBJXYFf\nAX8fEW8WO55ikHQ28FpEPFfsWEpEB2AwcFdEfAp4hxZqPtgbpW3ro0kS58eBAyR9pbhRlZZI7l1o\nkfsXnCBKhKSOJMlhWkT8utjxFNEw4IuS1gDTgRGS7i9uSEVVDVRHRH2N8mGShNFWnQH8d0TURMQ2\n4NfAZ4scUyl4VdLHANL311pio04QJUDJA4H/FVgeEf+32PEUU0RcFxG9I6KMpPNxTkS02V+IEfEK\nsF7SMWnR6cCyIoZUbOuAz0jqkv6/OZ023Gmf4zHgq+nnrwL/2RIbdYIoDcOAi0h+LS9MX/+j2EFZ\nyfgWME3SYmAQcEuR4ymatCb1MPA8sITkHNamht2Q9ADwDHCMpGpJXwN+AJwp6UWSWtYPWmRfHmrD\nzMyyuAZhZmaZnCDMzCyTE4SZmWVygjAzs0xOEGZmlskJwmw3JG3Pufx4oaQWu5NZUlnuqJxmpaRD\nsQMw2wu8GxGDih2EWWtzDcLsI5K0RtI/SVoi6c+SjkrLyyTNkbRY0mxJR6blh0l6RNKi9FU/RER7\nSf+SPuPg95I6p8t/O31GyGJJ04v0Na0Nc4Iw273OjZqYxubMq42I/sDPSEahBfgpcG9EDACmAXem\n5XcCT0bEQJLxlJam5UcDkyPieOAN4Ly0fCLwqXQ7Xy/UlzNriu+kNtsNSW9HRNeM8jXAiIhYnQ62\n+EpE9JC0CfhYRGxLyzdGRE9JNUDviPhrzjbKgJnpg16Q9D2gY0R8X9LvgLeBR4FHI+LtAn9Vs524\nBmG2Z6KJz83x15zP2/mgb/ALwGSS2sb89AE5Zq3GCcJsz4zNeX8m/fw0HzwGsxL4Q/p5NvANaHjm\n9kFNbVRSO6BPRMwFvgccBHyoFmNWSP5FYrZ7nSUtzJn+XUTUX+raPR1l9a/AhWnZt0ieAHctydPg\nxqflVwFT0tE3t5Mki41kaw/cnyYRAXf6UaPW2twHYfYRpX0QFRGxqdixmBWCm5jMzCyTaxBmZpbJ\nNQgzM8vkBGFmZpmcIMzMLJMThJmZZXKCMDOzTP8fyDe1IiJt2nwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Asu2M-o6QD",
        "colab_type": "text"
      },
      "source": [
        "# 4. Finetuning a Pretrained Model\n",
        "\n",
        "Models can be built from scratch as the example above illustrated using the simple model with very few layers. As the number of layers increases, the time needed to train the model will also increase substanially. If massive clusters of GPU resources and RAM resources are not available, then it may be impractical to train very large datasets using very deep models. \n",
        "\n",
        "A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image classification task.\n",
        "\n",
        "There are many pretrained networks available as part of the Keras framework. These include:\n",
        "\n",
        "\n",
        "1.   Xception\n",
        "2.   VGG16\n",
        "3.   VGG19\n",
        "4.   ResNet, ResNetV2\n",
        "5.   InceptionV3\n",
        "6.   InceptionResNetV2\n",
        "7.   MobileNet\n",
        "8.   MobileNetV2\n",
        "9.   DenseNet\n",
        "10.  NASNet \n",
        "\n",
        "A pretrained network consist of a trained convolutional base along with a trained classifier. A deeply trained convolutional base will have captured many aspects of the input data that can be generalized and used with a completely different classifier. For example, a pretrained network designed to classify images among 1000 different categories can be reused to detect objects it was never training on such as specific iamges of military tanks. \n",
        "\n",
        "This is accomplished by freezing the layers of the trained convolutional base so that backpropagation of the optimizer is not allowed to adjust the weights already learned. The trained classifier is then removed and a new classifier is added on top of the pretrained base. The new model is then trained, with adjustments only made to the classifier layers. \n",
        "\n",
        "If the results of the this new training is not sufficient, then the model can be fine tuned. This is accomplished by unfreezing the last few layers of the base model, and performing training again with more paramters able to be adjusted. The reason for not simply unfreezing all the layers is the computational resources (GPU, RAM) needed and the time it would take to train on deep layered models. \n",
        "\n"
      ]
    }
  ]
}